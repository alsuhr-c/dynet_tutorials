{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model in DyNet\n",
    "In this tutorial, we will train a bag-of-words model to differentiate between negative and postive movie reviews using the movie review dataset from Cornell [(Pang and Lee 2004)](http://www.cs.cornell.edu/home/llee/papers/cutsent.home.html).\n",
    "\n",
    "## Simple data pre-processing\n",
    "Loading the negative and positive reviews into memory. This code uses only the first sentence in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num per class: negative 1000; positive 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "def load_data(directory):\n",
    "    l = [ ]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            words = nltk.word_tokenize((open(directory + filename).readlines()[0]).lower())\n",
    "            l.append({\"id\" : filename, \"data\" : words})\n",
    "    return l\n",
    "\n",
    "negative_examples = load_data(\"neg/\")\n",
    "positive_examples = load_data(\"pos/\")\n",
    "\n",
    "print(\"num per class: negative \" + str(len(negative_examples)) + \"; positive \" + str(len(positive_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split into a train/dev split. For this dataset, there are not traditional splits (prior work uses cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths: train 1598; dev 402\n"
     ]
    }
   ],
   "source": [
    "dev_examples = [ ]\n",
    "train_examples = [ ]\n",
    "\n",
    "import random\n",
    "random.shuffle(negative_examples)\n",
    "random.shuffle(positive_examples)\n",
    "\n",
    "for example in negative_examples:\n",
    "    example[\"label\"] = 0\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "for example in positive_examples:\n",
    "    example[\"label\"] = 1\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "print(\"lengths: train \" + str(len(train_examples)) + \"; dev \" + str(len(dev_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'about an hour or so into `` the jackal , `` a character wandered around as people were being shot at in a big suspense sequence , and one of the audience members in the theatre i saw it in shouted out `` i hope she gets killed now ! ``'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train_examples[0][\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a vocabulary list using the words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7140"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set([word for example in train_examples for word in example[\"data\"]])) + [\"UNK\"]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 197, 6430, 1806, 4972, 2401, 1610, 6604, 573, 5763]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [vocab.index(word) for word in train_examples[0][\"data\"][:10] if word in vocab]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about', 'an', 'hour', 'or', 'so', 'into', '``', 'the', 'jackal', ',']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[index] for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we can start building the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the ParameterCollection\n",
    "Our model will be a simple bag-of-words model with a single hidden layer. The model will have the following parameters:\n",
    "\n",
    "1. A `LookupParameters` object for the words in the data, which is of size len(vocab) x 64.\n",
    "2. A `Parameters` object for the hidden layer of size 64 x 64.\n",
    "3. A `Parameters` object for the hidden layer's biases of size 64 x 1.\n",
    "4. A `Parameters` object for the final layer of size 64 x 2.\n",
    "5. A `Parameters` object for the final layer's biases of size 2 x 1.\n",
    "\n",
    "We will show off a few things in this section:\n",
    "\n",
    "1. Naming parameters. This will make things a bit easier to debug if necessary.\n",
    "2. Initializing parameters using [`PyInitializer`](http://dynet.readthedocs.io/en/latest/python_ref.html#parameters-initializers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/word-embeddings', (7140, 64)),\n",
       " ('/hidden-weights', (64, 64)),\n",
       " ('/hidden-biases', (64, 1)),\n",
       " ('/final-weights', (64, 2)),\n",
       " ('/final-biases', (2, 1))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import _dynet as dy\n",
    "dyparams = dy.DynetParams()\n",
    "dyparams.set_mem(2048)\n",
    "dyparams.set_autobatch(True)\n",
    "dyparams.init()\n",
    "\n",
    "pc = dy.ParameterCollection()\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_CLASSES = 2\n",
    "word_embeddings = pc.add_lookup_parameters((len(vocab), HIDDEN_SIZE), name=\"word-embeddings\")\n",
    "hidden_weights = pc.add_parameters((HIDDEN_SIZE, HIDDEN_SIZE), name=\"hidden-weights\", init=dy.NormalInitializer())\n",
    "hidden_biases = pc.add_parameters((HIDDEN_SIZE, 1), name=\"hidden-biases\", init=dy.NormalInitializer())\n",
    "final_weights = pc.add_parameters((HIDDEN_SIZE, NUM_CLASSES), name = \"final-weights\")\n",
    "final_biases = pc.add_parameters((NUM_CLASSES, 1), name = \"final-biases\")\n",
    "\n",
    "[(param.name(), param.shape()) for param in pc.lookup_parameters_list() + pc.parameters_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should implement a function that takes a single example and computes a set of scores over the two classes. This will later be used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75685036],\n",
       "       [0.77331418]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(example):\n",
    "    word_vectors = [ ]\n",
    "    \n",
    "    # First, get the word embeddings for every word in the data.\n",
    "    for word in example[\"data\"]:\n",
    "        if word in vocab:\n",
    "            word_vectors.append(word_embeddings[vocab.index(word)])\n",
    "        else:\n",
    "            word_vectors.append(word_embeddings[vocab.index(\"UNK\")])\n",
    "    \n",
    "    # Then get the average word embedding for the example.\n",
    "    embedding = dy.esum(word_vectors) / float(len(word_vectors))\n",
    "    \n",
    "    # Intermediate representation...\n",
    "    intermediate_value = dy.parameter(hidden_weights) * dy.reshape(embedding, (HIDDEN_SIZE, 1)) + dy.parameter(hidden_biases)\n",
    "    \n",
    "    # With a nonlinearity\n",
    "    intermediate_value = dy.tanh(intermediate_value)\n",
    "    \n",
    "    # Final probability distribution\n",
    "    scores = dy.transpose(dy.parameter(final_weights)) * intermediate_value  + dy.parameter(final_biases)\n",
    "    return scores\n",
    "\n",
    "dy.renew_cg()\n",
    "get_scores(train_examples[0]).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: loss, prediction\n",
    "\n",
    "To get the loss for a particular example, we can simply use the [`dy.pickneglogsoftmax`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.pickneglogsoftmax) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7014130353927612"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.pickneglogsoftmax(get_scores(train_examples[0]), train_examples[0][\"label\"]).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's write a simple function that will compute whether a prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(example):\n",
    "    prediction = np.argmax(get_scores(example).value())\n",
    "    return prediction == example[\"label\"]\n",
    "\n",
    "evaluate(train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Let's look at how training works for 20 examples. First we need to create an optimizer. Let's use the [Adam optimizer](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.AdamTrainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7014130353927612\n",
      "0.5896201729774475\n",
      "0.7085946798324585\n",
      "0.7329362034797668\n",
      "0.42414355278015137\n",
      "0.5898312926292419\n",
      "0.434162974357605\n",
      "0.8446372747421265\n",
      "0.5973041653633118\n",
      "0.509690523147583\n",
      "0.6853300333023071\n",
      "0.985931396484375\n",
      "0.5239347815513611\n",
      "0.5135217905044556\n",
      "0.5696007609367371\n",
      "0.5588383674621582\n",
      "0.5122370719909668\n",
      "0.4329153299331665\n",
      "0.5781350135803223\n",
      "0.6177761554718018\n",
      "total time: 0.20748400688171387\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "optimizer = dy.AdamTrainer(pc)\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(train_examples[:20]):\n",
    "    loss = dy.pickneglogsoftmax(get_scores(example), example[\"label\"])\n",
    "    loss.forward()\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    \n",
    "    print(loss.value())\n",
    "print(\"total time: \" + str(time.time() - epoch_start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "Batching your updates means that you consolidate a certain number of losses into a single value, then backpropagate over an average of the values. This has two affects on performance:\n",
    "\n",
    "1. Training could be a lot faster, especially with autobatching in DyNet. \n",
    "2. There are empirical changes in performance with lower or higher batch sizes. The batch size is a hyperparameter you can tune to get the best results in the end, and it depends a lot on the task you are using.\n",
    "\n",
    "The following code waits until a certain number of examples have been processed, and only then does an update. Importantly, we also turn on autobatching, so that intermediate computations will be batched automatically, making things faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0.4955019950866699\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "def epoch_train(examples):\n",
    "    dy.renew_cg()\n",
    "    random.shuffle(examples)\n",
    "    current_losses = [ ]\n",
    "    for i, example in enumerate(examples):\n",
    "        loss = dy.pickneglogsoftmax(get_scores(example), example[\"label\"])\n",
    "        current_losses.append(loss)\n",
    "\n",
    "        if len(current_losses) >= BATCH_SIZE:\n",
    "            mean_loss = dy.esum(current_losses) / float(len(current_losses))\n",
    "            mean_loss.forward()\n",
    "            mean_loss.backward()\n",
    "            optimizer.update()\n",
    "            current_losses = [ ]\n",
    "            dy.renew_cg()\n",
    "    if current_losses:\n",
    "        mean_loss = dy.esum(current_losses) / float(len(current_losses))\n",
    "        mean_loss.forward()\n",
    "        mean_loss.backward()\n",
    "        optimizer.update()\n",
    "       \n",
    "epoch_train(train_examples[:20])\n",
    "print(\"total time: \" + str(time.time() - epoch_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write some code that trains for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 accuracy: 0.47512437810945274\n",
      "improved!\n",
      "epoch 1 accuracy: 0.4701492537313433\n",
      "epoch 2 accuracy: 0.5223880597014925\n",
      "improved!\n",
      "epoch 3 accuracy: 0.5572139303482587\n",
      "improved!\n",
      "epoch 4 accuracy: 0.5348258706467661\n",
      "epoch 5 accuracy: 0.5522388059701493\n",
      "epoch 6 accuracy: 0.5447761194029851\n",
      "epoch 7 accuracy: 0.5522388059701493\n",
      "epoch 8 accuracy: 0.5671641791044776\n",
      "improved!\n",
      "epoch 9 accuracy: 0.5646766169154229\n",
      "epoch 10 accuracy: 0.5597014925373134\n",
      "epoch 11 accuracy: 0.5746268656716418\n",
      "improved!\n",
      "epoch 12 accuracy: 0.5796019900497512\n",
      "improved!\n",
      "epoch 13 accuracy: 0.5746268656716418\n",
      "epoch 14 accuracy: 0.5746268656716418\n",
      "epoch 15 accuracy: 0.5597014925373134\n",
      "epoch 16 accuracy: 0.5572139303482587\n",
      "epoch 17 accuracy: 0.5621890547263682\n",
      "epoch 18 accuracy: 0.5597014925373134\n",
      "epoch 19 accuracy: 0.5597014925373134\n",
      "epoch 20 accuracy: 0.5597014925373134\n",
      "epoch 21 accuracy: 0.5572139303482587\n",
      "epoch 22 accuracy: 0.5572139303482587\n",
      "epoch 23 accuracy: 0.554726368159204\n",
      "epoch 24 accuracy: 0.5621890547263682\n",
      "loading from model at epoch 12\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0.\n",
    "best_epoch = 0\n",
    "for i in range(25):\n",
    "    epoch_train(train_examples)\n",
    "    \n",
    "    accuracy = sum([float(evaluate(example)) for example in dev_examples]) / float(len(dev_examples))\n",
    "    print(\"epoch \" + str(i) + \" accuracy: \" + str(accuracy))\n",
    "    if accuracy > max_accuracy:\n",
    "        print(\"improved!\")\n",
    "        pc.save(\"model-epoch\" + str(i) + \".dy\")\n",
    "        best_epoch = i\n",
    "        max_accuracy = accuracy\n",
    "\n",
    "print(\"loading from model at epoch \" + str(best_epoch))\n",
    "pc.populate(\"model-epoch\" + str(best_epoch) + \".dy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has improved performance! Now we should do some analysis of its errors. \n",
    "\n",
    "## Error analysis\n",
    "First, we will print 20 examples of random errors that the model made and see if we can identify why they made the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\ta life less ordinary ( r ) while the extremely peculiar a life less ordinary does live up to its title , a more appropriate moniker would be a movie more misguided , for this confused , confusing attempt at romantic comedy is a most disarming disaster from the talented trainspotting team of director danny boyle , producer andrew macdonald , and screenwriter john hodge .\n",
      "0\tthe working title for no looking back was long time , nothing new , and rarely has there been a more apt name for a motion picture .\n",
      "0\ta silly film that tries to be a black comedy but plays more like lightweight comedy , with its main asset being a beautiful film location along spain 's mountainous coastline .\n",
      "1\tafter having heard so many critics describe `` return to me `` as an old-fashioned hollywood romance , i kept asking myself going into this movie : what exactly is an old-fashioned hollywood romance ?\n",
      "1\tby now i figured i 'd seen every alfred hitchcock film at least a half-dozen times -- not that i 'm complaining .\n",
      "0\tmovies do n't come much more ridiculously titled than `` i still know what you did last summer , `` but since the movie in question is best described as ridiculous , the title sort of works as a warning .\n",
      "1\t* * * * * * minor plot spoilers in review * * * * * * * * * * * * no major spoilers are in review * * * * * *\n",
      "1\tdid claus von bulow try to kill his wife sunny in their newport mansion ?\n",
      "1\twhat surprises me most about the long-awaited batman is that the movie centers not on the caped crusader but on the joker .\n",
      "1\tcapsule : suprisingly more of a comedy than a straight action flick , which is n't necessarily a bad thing .\n",
      "1\tdavid lynch 's `` blue velvet `` begins and ends with colorful , bright shots of flowers and happy americans mowing their lawns in a seemingly perfect american town .\n",
      "1\tsynopsis : as a response to accusations of sexual prejudice in the armed forces , a female naval intelligence officer is chosen to be a test case .\n",
      "0\ti had been looking forward to this film since i heard about it early last year , when matthew perry had just signed on .\n",
      "1\tin many ways , `` twotg `` does for tough-guy movies what la confidential did for police stories .\n",
      "1\tin `` the sweet hereafter , `` writer/director atom egoyan takes us beyond the tragedy of death into the tragedy of living .\n",
      "1\turban legend surprised me .\n",
      "0\tin china in 1982 i turned the tables on our national guide and asked him if he had any questions about america .\n",
      "0\tsynopsis : when a meteorite crashlands in the arizona desert , community college professors ira kane ( duchovny ) and harry block ( jones ) are first on the scene .\n",
      "0\tit 's tough to be an aspiring superhero in champion city .\n",
      "0\tvarious films seen at the seattle film festival :\n"
     ]
    }
   ],
   "source": [
    "wrong_examples = [example for example in dev_examples if not evaluate(example)]\n",
    "random.shuffle(wrong_examples)\n",
    "for example in wrong_examples[:20]:\n",
    "    print(str(example[\"label\"]) + \"\\t\" + \" \".join(example[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
