{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model in DyNet\n",
    "In this tutorial, we will train a bag-of-words model to differentiate between negative and postive movie reviews using the movie review dataset from Cornell [(Pang and Lee 2004)](http://www.cs.cornell.edu/home/llee/papers/cutsent.home.html).\n",
    "\n",
    "## Simple data pre-processing\n",
    "Loading the negative and positive reviews into memory. This code uses only the first sentence in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num per class: negative 1000; positive 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "def load_data(directory):\n",
    "    l = [ ]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            words = nltk.word_tokenize((open(directory + filename).readlines()[0]).lower())\n",
    "            l.append({\"id\" : filename, \"data\" : words})\n",
    "    return l\n",
    "\n",
    "negative_examples = load_data(\"neg/\")\n",
    "positive_examples = load_data(\"pos/\")\n",
    "\n",
    "print(\"num per class: negative \" + str(len(negative_examples)) + \"; positive \" + str(len(positive_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split into a train/dev split. For this dataset, there are not traditional splits (prior work uses cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths: train 1598; dev 402\n"
     ]
    }
   ],
   "source": [
    "dev_examples = [ ]\n",
    "train_examples = [ ]\n",
    "\n",
    "import random\n",
    "random.shuffle(negative_examples)\n",
    "random.shuffle(positive_examples)\n",
    "\n",
    "for example in negative_examples:\n",
    "    example[\"label\"] = 0\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "for example in positive_examples:\n",
    "    example[\"label\"] = 1\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "print(\"lengths: train \" + str(len(train_examples)) + \"; dev \" + str(len(dev_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"david schwimmer ( from the television series `` friends `` ) stars as a sensitive ( and slightly neurotic ) single guy who gets more than he expected from the grieving mother ( barbara hershey ) of a classmate he ca n't remember .\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train_examples[0][\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a vocabulary list using the words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7110"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set([word for example in train_examples for word in example[\"data\"]])) + [\"UNK\"]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6084, 4333, 4396, 5193, 581, 4065, 2184, 1192, 1462, 1192]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [vocab.index(word) for word in train_examples[0][\"data\"][:10] if word in vocab]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['david',\n",
       " 'schwimmer',\n",
       " '(',\n",
       " 'from',\n",
       " 'the',\n",
       " 'television',\n",
       " 'series',\n",
       " '``',\n",
       " 'friends',\n",
       " '``']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[index] for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we can start building the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the ParameterCollection\n",
    "Our model will be a simple bag-of-words model with a single hidden layer. The model will have the following parameters:\n",
    "\n",
    "1. A `LookupParameters` object for the words in the data, which is of size len(vocab) x 64.\n",
    "2. A `Parameters` object for the hidden layer of size 64 x 64.\n",
    "3. A `Parameters` object for the hidden layer's biases of size 64 x 1.\n",
    "4. A `Parameters` object for the final layer of size 64 x 2.\n",
    "5. A `Parameters` object for the final layer's biases of size 2 x 1.\n",
    "\n",
    "We will show off a few things in this section:\n",
    "\n",
    "1. Naming parameters. This will make things a bit easier to debug if necessary.\n",
    "2. Initializing parameters using [`PyInitializer`](http://dynet.readthedocs.io/en/latest/python_ref.html#parameters-initializers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/word-embeddings', (7110, 64)),\n",
       " ('/hidden-weights', (64, 64)),\n",
       " ('/hidden-biases', (64, 1)),\n",
       " ('/final-weights', (64, 2)),\n",
       " ('/final-biases', (2, 1))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import _dynet as dy\n",
    "dyparams = dy.DynetParams()\n",
    "dyparams.set_mem(2048)\n",
    "dyparams.set_autobatch(True)\n",
    "dyparams.init()\n",
    "\n",
    "pc = dy.ParameterCollection()\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_CLASSES = 2\n",
    "word_embeddings = pc.add_lookup_parameters((len(vocab), HIDDEN_SIZE), name=\"word-embeddings\")\n",
    "hidden_weights = pc.add_parameters((HIDDEN_SIZE, HIDDEN_SIZE), name=\"hidden-weights\", init=dy.NormalInitializer())\n",
    "hidden_biases = pc.add_parameters((HIDDEN_SIZE, 1), name=\"hidden-biases\", init=dy.NormalInitializer())\n",
    "final_weights = pc.add_parameters((HIDDEN_SIZE, NUM_CLASSES), name = \"final-weights\")\n",
    "final_biases = pc.add_parameters((NUM_CLASSES, 1), name = \"final-biases\")\n",
    "\n",
    "[(param.name(), param.shape()) for param in pc.lookup_parameters_list() + pc.parameters_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should implement a function that takes a single example and computes a set of scores over the two classes. This will later be used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2319001 ],\n",
       "       [-1.97725821]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(example):\n",
    "    word_vectors = [ ]\n",
    "    \n",
    "    # First, get the word embeddings for every word in the data.\n",
    "    for word in example[\"data\"]:\n",
    "        if word in vocab:\n",
    "            word_vectors.append(word_embeddings[vocab.index(word)])\n",
    "        else:\n",
    "            word_vectors.append(word_embeddings[vocab.index(\"UNK\")])\n",
    "    \n",
    "    # Then get the average word embedding for the example.\n",
    "    embedding = dy.esum(word_vectors) / float(len(word_vectors))\n",
    "    \n",
    "    # Intermediate representation...\n",
    "    intermediate_value = dy.parameter(hidden_weights) * dy.reshape(embedding, (HIDDEN_SIZE, 1)) + dy.parameter(hidden_biases)\n",
    "    \n",
    "    # With a nonlinearity\n",
    "    intermediate_value = dy.tanh(intermediate_value)\n",
    "    \n",
    "    # Final probability distribution\n",
    "    scores = dy.transpose(dy.parameter(final_weights)) * intermediate_value  + dy.parameter(final_biases)\n",
    "    return scores\n",
    "\n",
    "dy.renew_cg()\n",
    "get_scores(train_examples[0]).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: loss, prediction\n",
    "\n",
    "To get the loss for a particular example, we can simply use the [`dy.pickneglogsoftmax`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.pickneglogsoftmax) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03959619998931885"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.pickneglogsoftmax(get_scores(train_examples[0]), train_examples[0][\"label\"]).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's write a simple function that will compute whether a prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(example):\n",
    "    prediction = np.argmax(get_scores(example).value())\n",
    "    return prediction == example[\"label\"]\n",
    "\n",
    "evaluate(train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Let's look at how training works for 20 examples. First we need to create an optimizer. Let's use the [Adam optimizer](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.AdamTrainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03959619998931885\n",
      "0.043017446994781494\n",
      "0.05421280860900879\n",
      "0.04489290714263916\n",
      "0.052370548248291016\n",
      "0.03637802600860596\n",
      "0.045955777168273926\n",
      "0.05160510540008545\n",
      "0.035446763038635254\n",
      "0.04442179203033447\n",
      "0.043647170066833496\n",
      "0.03885173797607422\n",
      "0.03299403190612793\n",
      "0.03218638896942139\n",
      "0.04452073574066162\n",
      "0.04988741874694824\n",
      "0.05559486150741577\n",
      "0.03305995464324951\n",
      "0.033492207527160645\n",
      "0.03645956516265869\n",
      "total time: 0.20117998123168945\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "optimizer = dy.AdamTrainer(pc)\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(train_examples[:20]):\n",
    "    loss = dy.pickneglogsoftmax(get_scores(example), example[\"label\"])\n",
    "    loss.forward()\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    \n",
    "    print(loss.value())\n",
    "print(\"total time: \" + str(time.time() - epoch_start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "Batching your updates means that you consolidate a certain number of losses into a single value, then backpropagate over an average of the values. This has two affects on performance:\n",
    "\n",
    "1. Training could be a lot faster, especially with autobatching in DyNet. \n",
    "2. There are empirical changes in performance with lower or higher batch sizes. The batch size is a hyperparameter you can tune to get the best results in the end, and it depends a lot on the task you are using.\n",
    "\n",
    "The following code waits until a certain number of examples have been processed, and only then does an update. Importantly, we also turn on autobatching, so that intermediate computations will be batched automatically, making things faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0.17264699935913086\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "def epoch_train(examples):\n",
    "    dy.renew_cg()\n",
    "    random.shuffle(examples)\n",
    "    current_losses = [ ]\n",
    "    for i, example in enumerate(examples):\n",
    "        loss = dy.pickneglogsoftmax(get_scores(example), example[\"label\"])\n",
    "        current_losses.append(loss)\n",
    "\n",
    "        if len(current_losses) >= BATCH_SIZE:\n",
    "            mean_loss = dy.esum(current_losses) / float(len(current_losses))\n",
    "            mean_loss.forward()\n",
    "            mean_loss.backward()\n",
    "            optimizer.update()\n",
    "            current_losses = [ ]\n",
    "            dy.renew_cg()\n",
    "    if current_losses:\n",
    "        mean_loss = dy.esum(current_losses) / float(len(current_losses))\n",
    "        mean_loss.forward()\n",
    "        mean_loss.backward()\n",
    "        optimizer.update()\n",
    "       \n",
    "epoch_train(train_examples[:20])\n",
    "print(\"total time: \" + str(time.time() - epoch_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write some code that trains for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 accuracy: 0.4925373134328358\n",
      "improved!\n",
      "epoch 1 accuracy: 0.4925373134328358\n",
      "epoch 2 accuracy: 0.5174129353233831\n",
      "improved!\n",
      "epoch 3 accuracy: 0.5323383084577115\n",
      "improved!\n",
      "epoch 4 accuracy: 0.5870646766169154\n",
      "improved!\n",
      "epoch 5 accuracy: 0.5845771144278606\n",
      "epoch 6 accuracy: 0.5796019900497512\n",
      "epoch 7 accuracy: 0.5845771144278606\n",
      "epoch 8 accuracy: 0.5895522388059702\n",
      "improved!\n",
      "epoch 9 accuracy: 0.599502487562189\n",
      "improved!\n",
      "epoch 10 accuracy: 0.6044776119402985\n",
      "improved!\n",
      "epoch 11 accuracy: 0.599502487562189\n",
      "epoch 12 accuracy: 0.6019900497512438\n",
      "epoch 13 accuracy: 0.6119402985074627\n",
      "improved!\n",
      "epoch 14 accuracy: 0.6169154228855721\n",
      "improved!\n",
      "epoch 15 accuracy: 0.6094527363184079\n",
      "epoch 16 accuracy: 0.6069651741293532\n",
      "epoch 17 accuracy: 0.6069651741293532\n",
      "epoch 18 accuracy: 0.6069651741293532\n",
      "epoch 19 accuracy: 0.6019900497512438\n",
      "epoch 20 accuracy: 0.599502487562189\n",
      "epoch 21 accuracy: 0.599502487562189\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0.\n",
    "best_epoch = 0\n",
    "for i in range(25):\n",
    "    epoch_train(train_examples)\n",
    "    \n",
    "    accuracy = sum([float(evaluate(example)) for example in dev_examples]) / float(len(dev_examples))\n",
    "    print(\"epoch \" + str(i) + \" accuracy: \" + str(accuracy))\n",
    "    if accuracy > max_accuracy:\n",
    "        print(\"improved!\")\n",
    "        pc.save(\"model-epoch\" + str(i) + \".dy\")\n",
    "        best_epoch = i\n",
    "        max_accuracy = accuracy\n",
    "\n",
    "print(\"loading from model at epoch \" + str(best_epoch))\n",
    "pc.populate(\"model-epoch\" + str(best_epoch + \".dy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has improved performance! Now we should do some analysis of its errors. \n",
    "\n",
    "## Error analysis\n",
    "First, we will print 20 examples of random errors that the model made and see if we can identify why they made the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_examples = [example for example in dev_examples if not evaluate(example)]\n",
    "random.shuffle(wrong_examples)\n",
    "for example in wrong_examples[:20]:\n",
    "    print(str(example[\"label\"]) + \"\\t\" + \" \".join(example[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
