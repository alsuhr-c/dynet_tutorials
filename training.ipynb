{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model in DyNet\n",
    "In this tutorial, we will train a bag-of-words model to differentiate between negative and postive movie reviews using the movie review dataset from Cornell [(Pang and Lee 2004)](http://www.cs.cornell.edu/home/llee/papers/cutsent.home.html).\n",
    "\n",
    "## Simple data pre-processing\n",
    "Loading the negative and positive reviews into memory. This code uses only the first sentence in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num per class: negative 1000; positive 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "def load_data(directory):\n",
    "    l = [ ]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            words = nltk.word_tokenize((open(directory + filename).readlines()[0]).lower())\n",
    "            l.append({\"id\" : filename, \"data\" : words})\n",
    "    return l\n",
    "\n",
    "negative_examples = load_data(\"neg/\")\n",
    "positive_examples = load_data(\"pos/\")\n",
    "\n",
    "print(\"num per class: negative \" + str(len(negative_examples)) + \"; positive \" + str(len(positive_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split into a train/dev split. For this dataset, there are not traditional splits (prior work uses cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths: train 1617; dev 383\n"
     ]
    }
   ],
   "source": [
    "dev_examples = [ ]\n",
    "train_examples = [ ]\n",
    "\n",
    "import random\n",
    "random.shuffle(negative_examples)\n",
    "random.shuffle(positive_examples)\n",
    "\n",
    "for example in negative_examples:\n",
    "    example[\"label\"] = 0\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "for example in positive_examples:\n",
    "    example[\"label\"] = 1\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "print(\"lengths: train \" + str(len(train_examples)) + \"; dev \" + str(len(dev_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"well , here 's a distasteful , thoroughly amateurish item that , surprisingly , was actually a box-office hit at the time of its release .\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train_examples[0][\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a vocabulary list using the words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set([word for example in train_examples for word in example[\"data\"]])) + [\"UNK\"]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4411, 898, 5994, 1149, 2968, 6218, 898, 158, 2723, 6505]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [vocab.index(word) for word in train_examples[0][\"data\"][:10] if word in vocab]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " ',',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'distasteful',\n",
       " ',',\n",
       " 'thoroughly',\n",
       " 'amateurish',\n",
       " 'item']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[index] for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we can start building the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the ParameterCollection\n",
    "Our model will be a simple bag-of-words model with a single hidden layer. The model will have the following parameters:\n",
    "\n",
    "1. A `LookupParameters` object for the words in the data, which is of size len(vocab) x 64.\n",
    "2. A `Parameters` object for the hidden layer of size 64 x 64.\n",
    "3. A `Parameters` object for the hidden layer's biases of size 64 x 1.\n",
    "4. A `Parameters` object for the final layer of size 64 x 2.\n",
    "5. A `Parameters` object for the final layer's biases of size 2 x 1.\n",
    "\n",
    "We will show off a few things in this section:\n",
    "\n",
    "1. Naming parameters. This will make things a bit easier to debug if necessary.\n",
    "2. Initializing parameters using [`PyInitializer`](http://dynet.readthedocs.io/en/latest/python_ref.html#parameters-initializers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/word-embeddings', (7223, 64)),\n",
       " ('/hidden-weights', (64, 64)),\n",
       " ('/hidden-biases', (64, 1)),\n",
       " ('/final-weights', (64, 2)),\n",
       " ('/final-biases', (2, 1))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import _dynet as dy\n",
    "dyparams = dy.DynetParams()\n",
    "dyparams.set_mem(2048)\n",
    "dyparams.set_autobatch(True)\n",
    "dyparams.init()\n",
    "\n",
    "pc = dy.ParameterCollection()\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_CLASSES = 2\n",
    "word_embeddings = pc.add_lookup_parameters((len(vocab), HIDDEN_SIZE), name=\"word-embeddings\")\n",
    "hidden_weights = pc.add_parameters((HIDDEN_SIZE, HIDDEN_SIZE), name=\"hidden-weights\", init=dy.NormalInitializer())\n",
    "hidden_biases = pc.add_parameters((HIDDEN_SIZE, 1), name=\"hidden-biases\", init=dy.NormalInitializer())\n",
    "final_weights = pc.add_parameters((HIDDEN_SIZE, NUM_CLASSES), name = \"final-weights\")\n",
    "final_biases = pc.add_parameters((NUM_CLASSES, 1), name = \"final-biases\")\n",
    "\n",
    "[(param.name(), param.shape()) for param in pc.lookup_parameters_list() + pc.parameters_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should implement a function that takes a single example and computes a set of scores over the two classes. This will later be used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60171986],\n",
       "       [ 0.80132568]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(example):\n",
    "    word_vectors = [ ]\n",
    "    \n",
    "    # First, get the word embeddings for every word in the data.\n",
    "    for word in example[\"data\"]:\n",
    "        if word in vocab:\n",
    "            word_vectors.append(word_embeddings[vocab.index(word)])\n",
    "        else:\n",
    "            word_vectors.append(word_embeddings[vocab.index(\"UNK\")])\n",
    "    \n",
    "    # Then get the average word embedding for the example.\n",
    "    embedding = dy.esum(word_vectors) / float(len(word_vectors))\n",
    "    \n",
    "    # Intermediate representation...\n",
    "    intermediate_value = dy.parameter(hidden_weights) * dy.reshape(embedding, (HIDDEN_SIZE, 1)) + dy.parameter(hidden_biases)\n",
    "    \n",
    "    # With a nonlinearity\n",
    "    intermediate_value = dy.tanh(intermediate_value)\n",
    "    \n",
    "    # Final probability distribution\n",
    "    scores = dy.transpose(dy.parameter(final_weights)) * intermediate_value  + dy.parameter(final_biases)\n",
    "    return scores\n",
    "\n",
    "dy.renew_cg()\n",
    "get_scores(train_examples[0]).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: loss, prediction\n",
    "\n",
    "To get the loss for a particular example, we can simply use the [`dy.pickneglogsoftmax`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.pickneglogsoftmax) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6228612661361694"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.pickneglogsoftmax(get_scores(train_examples[0]), train_examples[0][\"label\"]).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's write a simple function that will compute whether a prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(example):\n",
    "    prediction = np.argmax(get_scores(example).value())\n",
    "    return prediction == example[\"label\"]\n",
    "\n",
    "evaluate(train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Let's look at how training works for 20 examples. First we need to create an optimizer. Let's use the [Adam optimizer](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.AdamTrainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6228612661361694\n",
      "1.5481170415878296\n",
      "1.7339640855789185\n",
      "1.8232612609863281\n",
      "1.8710105419158936\n",
      "1.4095180034637451\n",
      "1.7901537418365479\n",
      "1.0495200157165527\n",
      "1.658494234085083\n",
      "1.4216970205307007\n",
      "1.6540513038635254\n",
      "1.4352253675460815\n",
      "2.0071310997009277\n",
      "1.7645550966262817\n",
      "1.7766127586364746\n",
      "2.0397417545318604\n",
      "1.7066638469696045\n",
      "1.3489211797714233\n",
      "1.5554766654968262\n",
      "1.851932406425476\n",
      "total time: 0.11957287788391113\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "optimizer = dy.AdamTrainer(pc)\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(train_examples[:20]):\n",
    "    loss = dy.pickneglogsoftmax(get_scores(example), example[\"label\"])\n",
    "    loss.forward()\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    \n",
    "    print(loss.value())\n",
    "print(\"total time: \" + str(time.time() - epoch_start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "Batching your updates means that you consolidate a certain number of losses into a single value, then backpropagate over an average of the values. This has two affects on performance:\n",
    "\n",
    "1. Training could be a lot faster, especially with autobatching in DyNet. \n",
    "2. There are empirical changes in performance with lower or higher batch sizes. The batch size is a hyperparameter you can tune to get the best results in the end, and it depends a lot on the task you are using.\n",
    "\n",
    "The following code waits until a certain number of examples have been processed, and only then does an update. Importantly, we also turn on autobatching, so that intermediate computations will be batched automatically, making things faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0.21363592147827148\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "def epoch_train(examples):\n",
    "    dy.renew_cg()\n",
    "    random.shuffle(examples)\n",
    "    current_losses = [ ]\n",
    "    for i, example in enumerate(examples):\n",
    "        loss = dy.pickneglogsoftmax(get_scores(example), example[\"label\"])\n",
    "        current_losses.append(loss)\n",
    "\n",
    "        if len(current_losses) >= BATCH_SIZE:\n",
    "            mean_loss = dy.esum(current_losses) / float(len(current_losses))\n",
    "            mean_loss.forward()\n",
    "            mean_loss.backward()\n",
    "            optimizer.update()\n",
    "            current_losses = [ ]\n",
    "            dy.renew_cg()\n",
    "    if current_losses:\n",
    "        mean_loss = dy.esum(current_losses) / float(len(current_losses))\n",
    "        mean_loss.forward()\n",
    "        mean_loss.backward()\n",
    "        optimizer.update()\n",
    "       \n",
    "epoch_train(train_examples[:20])\n",
    "print(\"total time: \" + str(time.time() - epoch_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write some code that trains for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 accuracy: 0.4934725848563969\n",
      "improved!\n",
      "epoch 1 accuracy: 0.5326370757180157\n",
      "improved!\n",
      "epoch 2 accuracy: 0.5535248041775457\n",
      "improved!\n",
      "epoch 3 accuracy: 0.5483028720626631\n",
      "epoch 4 accuracy: 0.556135770234987\n",
      "improved!\n",
      "epoch 5 accuracy: 0.5744125326370757\n",
      "improved!\n",
      "epoch 6 accuracy: 0.5848563968668408\n",
      "improved!\n",
      "epoch 7 accuracy: 0.5822454308093995\n",
      "epoch 8 accuracy: 0.5822454308093995\n",
      "epoch 9 accuracy: 0.577023498694517\n",
      "epoch 10 accuracy: 0.587467362924282\n",
      "improved!\n",
      "epoch 11 accuracy: 0.597911227154047\n",
      "improved!\n",
      "epoch 12 accuracy: 0.6005221932114883\n",
      "improved!\n",
      "epoch 13 accuracy: 0.6031331592689295\n",
      "improved!\n",
      "epoch 14 accuracy: 0.5926892950391645\n",
      "epoch 15 accuracy: 0.608355091383812\n",
      "improved!\n",
      "epoch 16 accuracy: 0.6005221932114883\n",
      "epoch 17 accuracy: 0.608355091383812\n",
      "epoch 18 accuracy: 0.608355091383812\n",
      "epoch 19 accuracy: 0.6109660574412533\n",
      "improved!\n",
      "epoch 20 accuracy: 0.6031331592689295\n",
      "epoch 21 accuracy: 0.5953002610966057\n",
      "epoch 22 accuracy: 0.5953002610966057\n",
      "epoch 23 accuracy: 0.597911227154047\n",
      "epoch 24 accuracy: 0.597911227154047\n",
      "total training time: 150.27167797088623; 6.010867118835449 per epoch\n",
      "loading from model at epoch 19\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 25\n",
    "\n",
    "max_accuracy = 0.\n",
    "best_epoch = 0\n",
    "start_time = time.time()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    epoch_train(train_examples)\n",
    "    \n",
    "    accuracy = sum([float(evaluate(example)) for example in dev_examples]) / float(len(dev_examples))\n",
    "    print(\"epoch \" + str(i) + \" accuracy: \" + str(accuracy))\n",
    "    if accuracy > max_accuracy:\n",
    "        print(\"improved!\")\n",
    "        pc.save(\"model-epoch\" + str(i) + \".dy\")\n",
    "        best_epoch = i\n",
    "        max_accuracy = accuracy\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"total training time: \" + str(total_time) + \"; \" + str(float(total_time) / NUM_EPOCHS) + \" per epoch\")\n",
    "print(\"loading from model at epoch \" + str(best_epoch))\n",
    "pc.populate(\"model-epoch\" + str(best_epoch) + \".dy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has improved performance! Now we should do some analysis of its errors. \n",
    "\n",
    "## Evaluation and error analysis\n",
    "First, we will print 20 examples of random errors that the model made and see if we can identify why they made the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.39947780678851175\n",
      "0\tmy first exposure to the nightmare on elm street series of films was not this one , but in fact the third installment ( it was the first to gain a national theatrical release ) .\n",
      "0\tbruce willis needs to stay away from straightforward action pictures .\n",
      "1\tluckily , some people got starship troopers .\n",
      "0\tsteve martin is one of the funniest men alive .\n",
      "0\tit 's amazing how a comedian can have the some of the funniest stand-up around but completely fall flat in the movies .\n",
      "0\tmartial arts master steven seagal ( not to mention director ! ) has built a career out of playing an allegedly fictitious martial arts superman who never gets hurt in fights , talks in a hushed tone , and squints at any sign of danger .\n",
      "0\tfunny how your expectations can be defeated , and not in good ways .\n",
      "0\tposter boy for co-dependency needs patching\n",
      "1\tralph fiennes is carving out a nice niche for himself in the genre of period piece romances .\n",
      "0\t_dirty_work_ has a premise of deliciously mean-spirited potential .\n"
     ]
    }
   ],
   "source": [
    "wrong_examples = [example for example in dev_examples if not evaluate(example)]\n",
    "print(\"Error rate: \" + str(float(len(wrong_examples)) / len(dev_examples)))\n",
    "random.shuffle(wrong_examples)\n",
    "for example in wrong_examples[:10]:\n",
    "    print(str(example[\"label\"]) + \"\\t\" + \" \".join(example[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_input(input_string):\n",
    "    example = {\"data\": nltk.word_tokenize(input_string.lower())}\n",
    "    return np.argmax(get_scores(example).value())\n",
    "\n",
    "evaluate_input(\"This movie is really bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_input(\"This movie is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
