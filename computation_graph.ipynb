{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The DyNet computation graph\n",
    "Computation graphs are the fundamental language used in frameworks like TensorFlow, Theano, PyTorch, ... They describe a graph of mathematical operations. A computation graph is defined by a set of nodes and edges, where edges represent data (for example, a scalar value representing an input to the model or a matrix representing a set of learnable parameters) and nodes represent function calls (computations) (for example, multiplying a value by another value).\n",
    "\n",
    "The [class notes](http://www.cs.cornell.edu/courses/cs5740/2018sp/lectures/04-nn-compgraph.pdf) contain more information about computation graphs.\n",
    "\n",
    "## Entering/exiting the computation graph\n",
    "Below we learn how to put input data into the computation graph and perform a forward pass to get output data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import numpy as np\n",
    "\n",
    "my_scalar = np.random.randint(0,100)\n",
    "my_vector = np.random.random([3])\n",
    "my_matrix = np.random.random([3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some random data, let's put it into the DyNet computation graph. First, we have to renew the computation graph.\n",
    "\n",
    "A computational graph consists entirely of expressions, and it can't accept Numpy ndarrays or Python scalars. To input our data into a computational graph, we use the functions `scalarInput`, `inputVector` and `inputTensor` to create input expressions for each of the type of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.renew_cg()\n",
    "\n",
    "scalar_exp = dy.scalarInput(my_scalar)\n",
    "vector_exp = dy.inputVector(my_vector)\n",
    "matrix_exp = dy.inputTensor(my_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the size of these expressions using the [`dim`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.Expression.dim) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_exp.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_exp.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_exp.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed these inputs are a type of Expression, and can be used in a DyNet computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(matrix_exp))\n",
    "isinstance(matrix_exp, dy.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the data back by calling the [`value`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.Expression.value) function. Depending on the dimensions of your expression, it might return a float (if it's a scalar), a list (if it's a vector), or a numpy array (if it's a matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_exp.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_exp.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_exp.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create DyNet input expressions of any size by using several functions provided by DyNet, including [`zeros`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.zeros), [`ones`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.ones), and sampling from various random distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an expression containing matrix of zeros with DyNet\n",
    "zeros_exp1 = dy.zeros((3, 3))\n",
    "zeros_exp1.value()\n",
    "\n",
    "# Doing the same with Numpy\n",
    "zeros_np = np.zeros((3, 3))\n",
    "zeros_exp2 = dy.inputTensor(zeros_np)\n",
    "\n",
    "# The result is the same\n",
    "assert((zeros_exp1.value() == zeros_exp2.value()).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_uniform = dy.random_uniform((3, 3), -1.0, 1.0)\n",
    "random_uniform.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic mathematic operators\n",
    "\n",
    "Now we will learn about some of the basic math operators DyNet provides. We will also look at forward passes in the graph.\n",
    "\n",
    "It supports basic operators like exponentiating, trigonometric functions, and nonlinearities on any expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_exp.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.exp(matrix_exp).value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.tanh(matrix_exp).value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "dy.rectify(matrix_exp).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time `value` or `forward` is called, a forward pass is performed on the graph. This means that the  computations are actually carried out and a numerical value is returned. All nodes and edges in the graph that contribute to the value you request will be used. Before calling either of these functions, the graph is just a set of nodes and edges describing a computation. We will discuss forward passes more in detail during the batching section.\n",
    "\n",
    "We can check that without the `value` call, the result is not numeric, it is an `Expression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = dy.rectify(matrix_exp)\n",
    "a = expr * 10\n",
    "a.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DyNet has some simple binary operators overloaded, including +, -, \\*, and /. This means you can perform any of these operations with an Expression and a Python scalar, and the operation will be projected across all dimensions of the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(matrix_exp + 1.0).value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(matrix_exp / 2.0).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also provides component-wise operations on multiple expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(matrix_exp + random_uniform).value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.cdiv(matrix_exp, random_uniform).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing element-wise operations, you need to make sure the shapes match. DyNet automatically performs broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dy.ones((3,4)) + dy.ones((3,1))).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations are useful for summarizing information about an Expression or reshaping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.sum_elems(matrix_exp).value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.mean_elems(vector_exp).value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.reshape(matrix_exp, (9, 1)).dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few special operations can be used on lists of expressions. `esum` performs an element-wise sum on a list of expressions. This is useful for summing loss values for multiple training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy.esum([matrix_exp, random_uniform]).value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Collections and Parameters\n",
    "DyNet has a [`ParameterCollection`](http://dynet.readthedocs.io/en/latest/python_ref.html#parametercollection-and-parameters) object which is used to store optimizable tensors (e.g., a bias vector or weight matrix). \n",
    "\n",
    "[`Parameters`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.ParameterCollection) is a subclass of `Expresion` that contains optimizable tensor data. (http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.parameter).\n",
    "\n",
    "[`LookupParameters`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.LookupParameters) represents a table of parameters. In general, these are used as lists of vectors, where you can look up the appropriate vector and add it to the graph by indexing the lookup parameters as you would a normal Python list.\n",
    "\n",
    "Below we will see examples of the two types of parameters. First, we have to create the parameter collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = dy.ParameterCollection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a parameters object. Let's make a weight vector object that we can multiply out matrices with. When calling `add_parameters`, the parameters are automatically added to the computation graph, as well as stored in the `ParameterCollection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pc.add_parameters((3, 2))\n",
    "biases = pc.add_parameters((1, 2,), init=dy.UniformInitializer(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we created a parameters vector of size 1 x 2, loaded it into the graph, and got its value. We can also check the value by calling [`as_array`](http://dynet.readthedocs.io/en/latest/python_ref.html#dynet.Parameters.as_array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases.as_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a few computations in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = matrix_exp * weights + biases\n",
    "m2 = random_uniform * weights + biases\n",
    "result = dy.logistic(m1) + dy.logistic(m2)\n",
    "result.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that until we call `value`, no computations have actually been performed. \n",
    "\n",
    "Calling `backward` on a scalar `Expression` will perform the backward pass and compute gradients of the expression with respect to all parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_result = dy.sum_elems(result)\n",
    "scalar_result.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some lookup parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_parameters = pc.add_lookup_parameters((100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get values from the lookup parameters by using syntax similar to Python indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_vector = lookup_parameters[13]\n",
    "lookup_vector.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes during model development it's necessary to save and load learned parameters. DyNet will save all `Parameters` and `LookupParameters` objects. However, it won't save other things during training such as learning rate coefficients and optimizer parameters, so be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filename = \"save.dy\"\n",
    "\n",
    "pc.save(save_filename)\n",
    "pc.populate(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
