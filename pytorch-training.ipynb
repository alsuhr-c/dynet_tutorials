{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics\n",
    "\n",
    "In PyTorch, all data is stored in `Tensors`. These are just like Numpy ndarrays, but used in computational graphs. Tensors can be created directly, or from numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Creating Tensors from numpy arrays:\n",
    "matrix_np = np.random.normal(0, 0.1, (3,4))\n",
    "matrix_t = torch.from_numpy(matrix_np)\n",
    "matrix_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors from lists:\n",
    "matrix_t = torch.Tensor([[3.2, 4.5, 1.2], [1.2, 3.4, 1.2]])\n",
    "matrix_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tensors directly in PyTorch\n",
    "matrix_t = torch.rand(3, 4) # Uniform in [0,1] range\n",
    "matrix2_t = torch.normal(torch.Tensor([-1, 1]), torch.Tensor([[1.2, 1.2]])) # Normal\n",
    "print(matrix_t)\n",
    "print(matrix2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has many built-in math operations that can be performed on Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math operations:\n",
    "W = torch.normal(torch.zeros([10]), 0.01).view([5,2]) # 5x5 matrix, each element i.i.d. mean=0, stddev=0.01\n",
    "b = torch.zeros(5)\n",
    "x = torch.Tensor([1.0, 2.0])\n",
    "y = torch.sigmoid(torch.matmul(W, x) + b)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor has a shape (just like in numpy) and a number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"W.shape: {W.shape}\")\n",
    "print(f\"W.dim(): {W.dim()}\")\n",
    "print(f\"b.shape: {b.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors representing scalars have no shape, and zero-dimensions. These are called zero-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = torch.sum(y)\n",
    "print(f\"y_sum.shape: {y_sum.shape}\")\n",
    "print(f\"y_sum.dim(): {y_sum.dim()}\")\n",
    "y_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover a Python scalar from a zero-dimensional `Tensor`, we use the method `item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum.item()\n",
    "y_sum.detach() + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs in PyTorch\n",
    "\n",
    "PyTorch uses __eager execution__, which means it evaluates expressions on the computational graph __immediately__, as soon as the graph is built, but it keeps the graph in memory so that a backward pass can later be performed. DyNet  uses __lazy execution__ by default.\n",
    "\n",
    "This is great for debugging, because an error causes an Exception at exactly the line at which the error occured, making it easy to track down.\n",
    "\n",
    "**Careful:** When performing operations on the GPU, PyTorch is still eager, but CUDA is asynchronous. This means that operations on the GPU will __start__ running at the point at which they are called, but may finish later. Exceptions are thrown only at the next CUDA call. This behavior can be disabled by setting the environment variable CUDA_LAUNCH_BLOCKING=1.\n",
    "\n",
    "Each `Tensor` has an attribute `requires_grad`. If it is set to `True`, then PyTorch will compute gradients with respect to this `Tensor` during the backward pass. To demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 tensors:\n",
    "W = torch.normal(torch.zeros([10]), 0.01).view([5,2]) # 5x5 matrix, each element i.i.d. mean=0, stddev=0.01\n",
    "b = torch.zeros(5)\n",
    "x = torch.Tensor([1.0, 2.0])\n",
    "\n",
    "# Tell PyTorch that we want to compute gradients with respect to W\n",
    "W.requires_grad = True\n",
    "\n",
    "# Perform the computation\n",
    "y = torch.sigmoid(torch.matmul(W, x) + b)\n",
    "sum_y = torch.sum(y)\n",
    "\n",
    "# Run the backward pass (this can only be called on 0-dimensional tensors, a.k.a. scalars)\n",
    "sum_y.backward()\n",
    "\n",
    "# Print the gradients:\n",
    "print(f\"W gradients: {W.grad}\")\n",
    "print(f\"b gradients: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters in PyTorch\n",
    "\n",
    "Most functionality useful for neural network development is stored in `torch.nn` package.\n",
    "`torch.nn.Parameter` is a subclass of `Tensor` that is typically used to represent model parameters. It has 2 good features:\n",
    "* All `torch.nn.Parameter`s have `requires_grad=True` by default.\n",
    "* When used as attributes of `torch.nn.Module` classes, can be easily found and given to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "weights_parameter = Parameter(torch.zeros(5))\n",
    "weights_parameter.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data (copied from DyNet notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "def load_data(directory):\n",
    "    l = [ ]\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            words = nltk.word_tokenize((open(directory + filename).readlines()[0]).lower())\n",
    "            l.append({\"id\" : filename, \"data\" : words})\n",
    "    return l\n",
    "\n",
    "negative_examples = load_data(\"neg/\")\n",
    "positive_examples = load_data(\"pos/\")\n",
    "\n",
    "print(\"num per class: negative \" + str(len(negative_examples)) + \"; positive \" + str(len(positive_examples)))\n",
    "\n",
    "dev_examples = [ ]\n",
    "train_examples = [ ]\n",
    "\n",
    "import random\n",
    "random.shuffle(negative_examples)\n",
    "random.shuffle(positive_examples)\n",
    "\n",
    "for example in negative_examples:\n",
    "    example[\"label\"] = 0\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "for example in positive_examples:\n",
    "    example[\"label\"] = 1\n",
    "    randnum = random.random()\n",
    "    if randnum < 0.8:\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        dev_examples.append(example)\n",
    "        \n",
    "print(\"lengths: train \" + str(len(train_examples)) + \"; dev \" + str(len(dev_examples)))\n",
    "\n",
    "vocab = list(set([word for example in train_examples for word in example[\"data\"]])) + [\"UNK\"]\n",
    "print(f\"vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing simple neural network the PyTorch way\n",
    "\n",
    "We already saw how we can implement neural networks in PyTorch by writing the mathematical operations manually and then calling `backward` to compute gradients with respect to model parameters. This is great, but can get out of hand when building large models with millions or billions of parameters.\n",
    "\n",
    "PyTorch adopts a modular programming paradigm. Every neural network is a subclass of `nn.Module` base class. \n",
    "`nn.Module` represents a node of arbitrary complexity in a computational graph. Each `nn.Module` can internally consist of many other `nn.Module`s, allowing us to easily re-use building blocks.\n",
    "\n",
    "When subclassing `nn.Module`, we have to implement the `__init__` and `forward` methods. There is also the `backward` method, but it is already implemented for us.\n",
    "\n",
    "Let's implement a simple two-layer neural network `ReviewClassifier` and train it to classify online review sentiment. This is the same classifier as was used in the `DyNet` tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes, vocab_size):\n",
    "        super(ReviewClassifier, self).__init__() # Unfortunately we have to call the superclass __init__ method\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "        \n",
    "    def forward(self, tokenized_sentence):\n",
    "        word_embeds = self.embeddings(tokenized_sentence)\n",
    "        sentence_embed = torch.mean(word_embeds, dim=1)\n",
    "        h = self.layer1(sentence_embed)\n",
    "        h = torch.tanh(h)\n",
    "        out = self.layer2(h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing parameters\n",
    "\n",
    "Let's train our ReviewClassifier on the same task - online review classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "model = ReviewClassifier(hidden_size=HIDDEN_SIZE, num_classes=NUM_CLASSES, vocab_size=len(vocab))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(train_examples[:20]):\n",
    "    \n",
    "    # First, tokenize the input sentence\n",
    "    tok_sentence = []\n",
    "    for word in example[\"data\"]:\n",
    "        if word in vocab:\n",
    "            tok_sentence.append(vocab.index(word))\n",
    "        else:\n",
    "            tok_sentence.append(vocab.index(\"UNK\"))\n",
    "    \n",
    "    # Convert the sentence and labels to Tensors of type long.\n",
    "    tok_sentence = torch.Tensor(tok_sentence).long()\n",
    "    label = torch.Tensor([example[\"label\"]]).long()\n",
    "    \n",
    "    # Add an additional batch dimension\n",
    "    tok_sentence = tok_sentence[np.newaxis, :]\n",
    "    \n",
    "    # Run the forward pass and compute loss\n",
    "    score = model(tok_sentence)\n",
    "    loss = criterion(score, label)\n",
    "    \n",
    "    # Zero the gradients, run backward pass, and take a gradient step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss.item())\n",
    "print(\"total time: \" + str(time.time() - epoch_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "In PyTorch with eager execution, we need to manually pack our training examples into batches before feeding them to the network.\n",
    "\n",
    "Most operations in `torch.nn` assume that the first dimension of all input and output tensors is the batch dimension.  Basic math operations in `torch.tensor` or `torch` do not make this assumption.\n",
    "\n",
    "To use batching for the review classifier, we need to stack together multiple reviews into a single tensor, so that we can simultaneously apply our model on all reviews in the batch.\n",
    "\n",
    "Different reviews have different lengths. We can't just stack them together into a single tensor.\n",
    "\n",
    "What we can do, is to make sure there is space for the longest review, and simply pad the shorter reviews with zeros. Then we need to keep track of how long each review is, so that we avoid treating these padded zeros as words that are part of the review. A simple way to do this is by using a mask, where 1 indicates that the corresponding token (word) is part of a review and 0 indicates that it is a padded value and should be ignored.\n",
    "\n",
    "An alternative way to do this is to keep a list of review lengths around, but the masking approach is easier in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifierWithBatching(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes, vocab_size):\n",
    "        super(ReviewClassifierWithBatching, self).__init__() # Unfortunately we have to call the superclass __init__ method\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "        \n",
    "    def forward(self, tokenized_sentences, mask):\n",
    "        # tokenized_sentences is a tensor of size BxL where L is the length of the longest review\n",
    "        # mask is also a tensor of size BxL, but only stores zeros and ones, indicating whether the \n",
    "        #  corresponding word in tokenized_sentences is a real word and part of a review, or a padded value.\n",
    "        word_embeds = self.embeddings(tokenized_sentences)\n",
    "        \n",
    "        # multiplying word_embeds with the mask zeroes out the word embeddings corresponding to padded zeroes\n",
    "        sentence_embed = torch.sum(word_embeds * mask[:, :, np.newaxis], dim=1) / \\\n",
    "            (torch.sum(mask, 1)[:, np.newaxis] + 1e-32)\n",
    "        \n",
    "        h = self.layer1(sentence_embed)\n",
    "        out = self.layer2(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to make changes to our data pre-processing code, to create batches and masks. We create a new function examples_to_batch that takes as input multiple training examples from our dataset, and produces a batch of tokenized input reviews, labels and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "def examples_to_batch(batch_of_examples):\n",
    "    sentence_list = []\n",
    "    label_list = []\n",
    "    max_len = 0\n",
    "\n",
    "    for example in batch_of_examples:\n",
    "        # First, tokenize the input review\n",
    "        tok_sentence = []\n",
    "        for word in example[\"data\"]:\n",
    "            if word in vocab:\n",
    "                tok_sentence.append(vocab.index(word))\n",
    "            else:\n",
    "                tok_sentence.append(vocab.index(\"UNK\"))\n",
    "\n",
    "        # Update length of longest sentence\n",
    "        if len(tok_sentence) > max_len:\n",
    "            max_len = len(tok_sentence)\n",
    "        \n",
    "        # Convert the review and labels to Tensors of type long and add to the list\n",
    "        tok_sentence = torch.Tensor(tok_sentence).long()\n",
    "        sentence_list.append(tok_sentence)\n",
    "        label = torch.Tensor([example[\"label\"]]).long()\n",
    "        label_list.append(label)\n",
    "    \n",
    "    # Stack all labels into a batch\n",
    "    batch_of_labels = torch.cat(label_list)\n",
    "    # For reviews it's not that easy since they have variable length.\n",
    "    # Create a new tensor that has enough space\n",
    "    batch_of_sentences = torch.zeros([len(sentence_list), max_len]).long()\n",
    "    mask = torch.zeros([len(sentence_list), max_len])\n",
    "    for i,sentence in enumerate(sentence_list):\n",
    "        batch_of_sentences[i,:len(sentence)] = sentence\n",
    "        mask[i, :len(sentence)] = 1.0\n",
    "    \n",
    "    return batch_of_sentences, batch_of_labels, mask\n",
    "    \n",
    "\n",
    "def epoch_train(train_examples):\n",
    "    epoch_start_time = time.time()\n",
    "    random.shuffle(train_examples)\n",
    "    for i in range(0, len(train_examples), BATCH_SIZE):\n",
    "        batch_of_examples = train_examples[i:i+BATCH_SIZE]\n",
    "        reviews, labels, mask = examples_to_batch(batch_of_examples)\n",
    "\n",
    "        # Run the forward pass and compute loss\n",
    "        scores = model(reviews, mask)\n",
    "        loss = criterion(scores, labels)\n",
    "\n",
    "        # Zero the gradients, run backward pass, and take a gradient step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(loss.item())\n",
    "    print(\"total time: \" + str(time.time() - epoch_start_time))\n",
    "    \n",
    "def evaluate_accuracy(dev_examples):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    for i in range(0, len(dev_examples), BATCH_SIZE):\n",
    "        batch_of_examples = dev_examples[i:i+BATCH_SIZE]\n",
    "        sentences, labels, mask = examples_to_batch(batch_of_examples)\n",
    "        scores = model(sentences, mask)\n",
    "        \n",
    "        num_correct += sum(np.argmax(scores.detach().numpy(), 1) == labels.numpy())\n",
    "        num_total += len(batch_of_examples)\n",
    "    return float(num_correct) / (num_total + 1e-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 25\n",
    "\n",
    "model = ReviewClassifierWithBatching(hidden_size=HIDDEN_SIZE, num_classes=NUM_CLASSES, vocab_size=len(vocab))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "max_accuracy = 0.\n",
    "best_epoch = 0\n",
    "start_time = time.time()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    epoch_train(train_examples)\n",
    "    accuracy = evaluate_accuracy(dev_examples)\n",
    "    print(\"epoch \" + str(i) + \" accuracy: \" + str(accuracy))\n",
    "    if accuracy > max_accuracy:\n",
    "        print(\"improved!\")\n",
    "        with open(\"model-epoch\" + str(i) + \".pytorch\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "        best_epoch = i\n",
    "        max_accuracy = accuracy\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"total training time: \" + str(total_time) + \"; \" + str(float(total_time) / NUM_EPOCHS) + \" per epoch\")\n",
    "print(\"loading from model at epoch \" + str(best_epoch))\n",
    "with open(\"model-epoch\" + str(best_epoch) + \".pytorch\", \"rb\") as fp:\n",
    "    model.load_state_dict(torch.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
